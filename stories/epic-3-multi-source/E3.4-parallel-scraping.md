# E3.4: Parallel Scraping with Timeout

**Epic:** E3 - Multi-Source Integration
**Story ID:** E3.4
**Story Points:** 3
**Priority:** P1 (High)
**Type:** Feature
**Status:** âœ… Completed

---

## User Story

**As a** developer
**I want to** scrape multiple sources concurrently
**So that** total execution time is minimized

---

## Acceptance Criteria

- [x] Update `EventOrchestrator` to use `concurrent.futures.ThreadPoolExecutor`
- [x] Launch scraper threads in parallel (max workers = # sources)
- [x] Set timeout per source (30 seconds default, configurable)
- [x] If source times out: Log warning, continue with other sources
- [x] If source errors: Log error, continue with other sources
- [x] Aggregate results from successful sources
- [x] Log execution summary (which sources succeeded/failed, total time)
- [x] Integration test: Mock slow source, verify timeout works

---

## Expected Behavior

```bash
$ python -m src.orchestrator --sources knco,library,county
[INFO] Scraping 3 sources in parallel...
[INFO] knco completed in 2.3s (200 events)
[INFO] library completed in 1.8s (30 events)
[WARN] county timed out after 30s (0 events)
[SUCCESS] Total: 230 events from 2/3 sources
```

---

## Dependencies

**Depends on:** E3.1 (Library Scraper), E3.2 (County Scraper)
**Blocks:** None (can run in parallel with E3.5)

---

## Definition of Done

- [x] Multiple scrapers run in parallel
- [x] Timeout prevents hanging
- [x] Graceful failure for individual sources
- [x] Total execution time < 30 seconds
- [x] Integration test passes

---

## Implementation Summary

**Files Modified:**
- `src/config.py` - Added `SCRAPER_TIMEOUT` configuration
- `src/orchestrator.py` - Implemented parallel scraping with ThreadPoolExecutor
  - Added `_fetch_single_source()` method for isolated source execution
  - Updated `fetch_events()` to support parallel mode
  - Added timeout handling and graceful failure
  - Enhanced CLI with `--sources`, `--no-parallel`, and `--timeout` flags
- `tests/test_orchestrator.py` - Added comprehensive tests for parallel scraping, timeout handling, and failure scenarios

**New Features:**
- Parallel scraping using ThreadPoolExecutor (default: enabled)
- Configurable per-source timeout (default: 30s)
- Graceful failure handling (continue with successful sources)
- Detailed execution summary logging
- CLI flags: `--sources knco,library,county`, `--no-parallel`, `--timeout 30`

**Test Coverage:**
- Parallel execution with multiple sources
- Timeout handling for slow sources
- Sequential mode (when parallel is disabled)
- Graceful failure when one source fails

---

## Technical Notes

Use Python's `concurrent.futures.ThreadPoolExecutor`:
```python
from concurrent.futures import ThreadPoolExecutor, TimeoutError

with ThreadPoolExecutor(max_workers=3) as executor:
    futures = {executor.submit(scraper.scrape): scraper.name
               for scraper in scrapers}
    # Wait with timeout, handle results
```

---

**Created:** 2025-10-07
**Last Updated:** 2025-10-07
**Completed:** 2025-10-07
